{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [02:16, 7342.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = codecs.open('./wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoStem(message):\n",
    "    return [removeNumeric(stripEmoji(singleCharacterRemove(removePunctuation\n",
    "                                                           (removeHyperlinks\n",
    "                                                            (removeHashtags\n",
    "                                                             (removeUsernames\n",
    "                                                              (stemWord(word)))))))) for word in message.split()]\n",
    "def stemWord(tweet):\n",
    "    return tweet.lower()\n",
    "\n",
    "def removeUsernames(tweet):\n",
    "    return re.sub('@[^\\s]+', '', tweet)\n",
    "\n",
    "def removeHashtags(tweet):\n",
    "    return re.sub(r'#[^\\s]+', '', tweet)\n",
    "\n",
    "def removeHyperlinks(tweet):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
    "\n",
    "def removeNumeric(value):\n",
    "    blist2 = [item for item in value if not item.isdigit()]\n",
    "    blist3 = \"\".join(blist2)\n",
    "    return blist3\n",
    "\n",
    "def removePunctuation(tweet):\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]','',tweet)\n",
    "\n",
    "def singleCharacterRemove(tweet):\n",
    "    return re.sub(r'(?:^| )\\w(?:$| )', ' ', tweet)\n",
    "\n",
    "def stripEmoji(text):\n",
    "\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('./tweets.csv', sep=',', header=0)\n",
    "tweet_list = tweets_df['Tweet'].tolist()\n",
    "label_list = tweets_df['Label'].tolist()\n",
    "one_hot_labels = keras.utils.to_categorical(label_list, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 4297.15it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_list = []\n",
    "for sentence in tqdm(tweet_list):\n",
    "    tokens = \" \".join(splitIntoStem(sentence)).split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_list.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"tweet\"] = processed_list\n",
    "df['doc_len'] = df['tweet'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(processed_list, one_hot_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  395\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train + x_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([112,   7,  14,   6,  12,   8,   5,  14,   9,   5,   9,   4,  55,\n",
       "         7,  13,   9,   4,   5,  14,  19,  13,  15,  12,   7,  13,   6,\n",
       "         9,  14,  17,   7])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in word_seq_train + word_seq_test]\n",
    "num_tokens = np.array(num_tokens)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_test = word_seq_test[:-1]\n",
    "y_test = y_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 4\n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 23\n",
      "sample words not found:  ['yagerûªs' 'redford' 'jarre' 'pridechoose' 'lilian' 'oconnors'\n",
      " 'pridechoose' 'jeanmichel' 'redford' 'jarre']\n"
     ]
    }
   ],
   "source": [
    "words_not_found = []\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 35, 300)           118500    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 35, 64)            96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 17, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 17, 64)            20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 237,320\n",
      "Trainable params: 118,820\n",
      "Non-trainable params: 118,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(num_filters, 5, padding='same', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.8114 - acc: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0d9dda198>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 999us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.393572449684143, 0.20000000298023224]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(word_seq_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
