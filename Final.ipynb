{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from httplib2 import Http\n",
    "import oauth2client\n",
    "from oauth2client import file, client, tools\n",
    "obj = lambda: None\n",
    "lmao = {\"auth_host_name\":'localhost', 'noauth_local_webserver':'store_true', 'auth_host_port':[8080, 8090], 'logging_level':'ERROR'}\n",
    "for k, v in lmao.items():\n",
    "    setattr(obj, k, v)\n",
    "    \n",
    "# authorization boilerplate code\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive.readonly'\n",
    "store = file.Storage('token.json')\n",
    "creds = store.get()\n",
    "# The following will give you a link if token.json does not exist, the link allows the user to give this app permission\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('client_id.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 4%.\n",
      "Download 9%.\n",
      "Download 13%.\n",
      "Download 18%.\n",
      "Download 23%.\n",
      "Download 27%.\n",
      "Download 32%.\n",
      "Download 37%.\n",
      "Download 41%.\n",
      "Download 46%.\n",
      "Download 51%.\n",
      "Download 55%.\n",
      "Download 60%.\n",
      "Download 64%.\n",
      "Download 69%.\n",
      "Download 74%.\n",
      "Download 78%.\n",
      "Download 83%.\n",
      "Download 88%.\n",
      "Download 92%.\n",
      "Download 97%.\n",
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "DRIVE = discovery.build('drive', 'v3', http=creds.authorize(Http()))\n",
    "# if you get the shareable link, the link contains this id, replace the file_id below\n",
    "file_id = '1Vf8aByqXQwTVmxxs5jKenvAXaxSF6TsH'\n",
    "request = DRIVE.files().get_media(fileId=file_id)\n",
    "# replace the filename and extension in the first field below\n",
    "fh = io.FileIO('wiki-news-300d-1M.vec', mode='w')\n",
    "downloader = MediaIoBaseDownload(fh, request)\n",
    "done = False\n",
    "while done is False:\n",
    "    status, done = downloader.next_chunk()\n",
    "    print(\"Download %d%%.\" % int(status.progress() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [02:13, 7487.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = codecs.open('./wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoStem(message):\n",
    "    return [removeNumeric(stripEmoji(singleCharacterRemove(removePunctuation\n",
    "                                                           (removeHyperlinks\n",
    "                                                            (removeHashtags\n",
    "                                                             (removeUsernames\n",
    "                                                              (stemWord(word)))))))) for word in message.split()]\n",
    "def stemWord(tweet):\n",
    "    return tweet.lower()\n",
    "\n",
    "def removeUsernames(tweet):\n",
    "    return re.sub('@[^\\s]+', '', tweet)\n",
    "\n",
    "def removeHashtags(tweet):\n",
    "    return re.sub(r'#[^\\s]+', '', tweet)\n",
    "\n",
    "def removeHyperlinks(tweet):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
    "\n",
    "def removeNumeric(value):\n",
    "    blist2 = [item for item in value if not item.isdigit()]\n",
    "    blist3 = \"\".join(blist2)\n",
    "    return blist3\n",
    "\n",
    "def removePunctuation(tweet):\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]','',tweet)\n",
    "\n",
    "def singleCharacterRemove(tweet):\n",
    "    return re.sub(r'(?:^| )\\w(?:$| )', ' ', tweet)\n",
    "\n",
    "def stripEmoji(text):\n",
    "\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('./tweets.csv', sep=',', header=0)\n",
    "tweet_list = tweets_df['Tweet'].tolist()\n",
    "label_list = tweets_df['Label'].tolist()\n",
    "one_hot_labels = keras.utils.to_categorical(label_list, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 3348.66it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_list = []\n",
    "for sentence in tqdm(tweet_list):\n",
    "    tokens = \" \".join(splitIntoStem(sentence)).split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_list.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"tweet\"] = processed_list\n",
    "df['doc_len'] = df['tweet'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(df['doc_len'].mean() + df['doc_len'].std()).astype(int)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(processed_list, one_hot_labels, test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  395\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train + x_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 12,  14,  12,   5,   6, 112,   7,   5,   9,   5,  14,   9,   7,\n",
       "         7,   6,   8,   7,  55,  13,  15,   4,  14,   4,  13,  17,   9,\n",
       "         9,  13,  14,  19])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in word_seq_train + word_seq_test]\n",
    "num_tokens = np.array(num_tokens)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_test = word_seq_test[:-1]\n",
    "y_test = y_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 4\n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null word embeddings: 23\n",
      "sample words not found:  ['bookershortlisted' 'jehovahs' 'neeson' 'pridechoose' 'madeley'\n",
      " 'yagerûªs' 'madeley' 'jarre' 'diaghilev' 'leahycrapo']\n"
     ]
    }
   ],
   "source": [
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 35, 300)           118500    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 35, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 35, 64)            96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 17, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 17, 64)            20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 237,320\n",
      "Trainable params: 118,820\n",
      "Non-trainable params: 118,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(num_filters, 5, padding='same', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.6308 - acc: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x277fa6df588>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(word_seq_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9873528480529785, 0.699999988079071]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(word_seq_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
